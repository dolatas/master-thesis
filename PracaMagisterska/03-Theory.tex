\chapter{Podłoże teoretyczne}
\label{c3}

\section{Wstęp}
\label{c31}
Kolejny rozdział przedstawia aktualne metody i istniejące algorytmy związane z tematem pracy. Poza podstawowym algorytmem Apriori (\cite{Agrawal}), który używa drzew haszowych do przechwywania kandydatów, opisano trzy modyfikacje tego alogrytmu. Główna różnica polega na tym, że wykorzystują one inną strukturę, a mianowicie drzewa prefiksowe. Są to rozwiązania zaproponowane przez Christina Borgelta (\cite{Borgelt}), Ferenca Bodona (\cite{Bodon}) oraz Barta Goethalsa (\cite{Goethals}). Ze względu na wykorzystanie prostszej struktury okazały się one szybsze od standardowego algorytmu.
 
Innym problemem jest optymalizacja wykonania kilku zadań Apriori uruchomionych współbieżnie na nakładających się podzbiorach tabeli z danymi. Metody z tym związane to Common Counting (\cite{WojciechowskiCC}) i Common Candidate Tree (\cite{WojciechowskiCCT}). Oparte są one o implementację Apriori z zastosowaniem drzew haszowych. Brakuje jednak adaptacji tych algorytmów, polegającej na zmianie struktury na drzewa prefiksowe. Właśnie taka modyfikacja została wprowadzona, a uzyskane efekty opisano w kolejnych rozdziałach niniejszej pracy. 


\section{Przegląd istniejących rozwiązań}
\label{c32}

\subsection{Algorytm Apriori \cite{Agrawal}}
\label{c321}
Algorytm Apriori jest algorytmem eksploracji poziomej. Szuka zbiorów częstych o rozmiarach \(1, 2,\dots , k\). Algorytm rozpoczyna od zbiorów o rozmiarze 1 i następnie zwiększa ten rozmiar w kolejnych iteracjach. Elementy każdej transakcji są uporządkowane leksykograficznie - jeżeli nawet transakcje nie są posortowane, to krokiem wstępnym algorytmu może być leksykograficzne uporządkowanie elementów transakcji (\cite{Morzy}). Po pierwszym kroku zebrane są zatem wszystkie elementy występujące w transakcjach (w postaci zbiorów jednolementowych). Następnie sprawdzane jest, które z nich posiadają wsparcie nie mniejsze niż \(minsup\). Elementy niespełniające tego wymgania są odrzucane. Pozostałe służą do utworzenia dwuelementowych zbiorów kandydujących (ang. \textit{candidate itemsets}). Dla wygenerowanych zbiorów spradzane jest czy posiadają wsparcie równe co najmniej \(minsup\). Jeśli tak, to taki zbiór jest dodawany do listy zbiorów częstych i w kolejnej iteracji jest wykorzystywany (wraz z innymi zbiorami z tejże listy) do generowania zbiorów kandydatów o rozmiarze o 1 większym. Wsparcie zbiorów sprawdzane jest na podstawie odczytu danych z bazy danych. Algorytm zatrzymuje się gdy nie ma już możliwości generowania kolejnych zbiorów. W wyniku jego działania zwracana jest suma \(k\)-elementowych zbiorów częstych \((k = 1, 2,\dots)\), która może zostać wykozystana do generowania reguł asocjacyjnych.

\subsection{Algorytm Apriori - implementacja Christina Borgelta \cite{Borgelt}}
\label{c322}

\subsection{Algorytm Apriori - implementacja Ferenca Bodona \cite{Bodon}}
\label{c323}

\subsection{Algorytm Apriori - implementacja Barta Goethalsa \cite{Goethals}}
\label{c324}


\subsection{Common Counting \cite{WojciechowskiCC}}
\label{c325}
W metodzie Common Counting chodzi o równoległe wykonanie zbioru zapytań eksploracyjnych algorytmem Apriori z integracją porywających się fragmentów bazy danych. Na wejściu algorytm otrzymuje zbiór elementarnych predykatów selekcji danych dla zbioru zapytań eksploracyjnych \(DMQ\). Początkowo algorytm ustala zbiór wszystkich elementów, czyli takich, które wystąpiły w co najmniej jednej transakcji. W kolejnych krokach generowane są zbiory częste odzielnnie dla każdego z zapytań. Przebiega to w taki sam sposób jak w przypadku standardowego alogrytmu Apriori. Z każdym zapytaniem powiązane jest drzewo haszowe, w którym przechowywani są kandydaci. Warunek zatrzymania algorytmu jest taki jak w standardowym Apriori (brak możliwości wygenerowania kandyadtów w kolejnej iteracji), z tą różnicą, że musi być spełniony dla wszystkich zapytań ze zbioru. 
Zliczenie wystąpień kandyadatów jest realizowane dla wszystkich zapytań jednocześnie. Partycje bazy danych są odczytywane sekwencyjnie dla poszczególnych elementarnych predykatów selekcji danych. Powiększeniu ulegają liczniki kandydatów zawartych w analizowanej transakcji dla zapytań posiadającyh odwołania do danej partycji. Lista kandydatów zawiarających się w danej transakcji ustalana jest poprzez testowanie transakcji względem drzew haszowych. Należy tutaj zaznaczyć, że w przypadku gdzy kilka zapytań współdzieli dany elementarny predykat selekcji danych, to podczas zliczeń wystąpień kandydatów odczyt właściwej mu partycji jest wykonywany tylko raz. Zatem optymalizowane są odczyty współdzielonych przez zapytania fragmentów bazy danych, przy czym pozostałe kroki algorytmu Apriori pozostają niezmienione i są wykonywane oddzielnie dla każdego zapytania. 

\subsection{Common Candidate Tree \cite{WojciechowskiCCT}}
\label{c326}
Common Candidate Tree podobna do Common Counting. Również korzysta z oryginalnego Apriori i wykorzystuje strukturę drzewa haszowego. Różnica polega na tym, że zwiększony został stopień współbieżności przetwarzania. Uzyskano to dzięki współdzieleniu pamięciowej struktury drzewa składującego kandydatów. Jest to duża zaleta w porównaniu z Common Counting, gdyż - zamiast wielu - tworzone jest jedno drzewo haszowe o niezmiennej strukturze. Poza zachowaniem integracji odczytów współdzielonych możliwa jest integracja testowania czy w danej transakcji zawierają się kandydaci z poszczególnych zapytań. Realizacja tego alogrytmu wymagała rozszerzenia struktury kandydatów. W jej wyniku z każdym kandydatem związany został wektor liczników (jeden licznik dla jednego zapytania), a nie pojedyńczy licznik. Dodatkowo - dla rozróżnienia zapytań, które wygenerowały danego kandydata - dołączony został wektor flag logicznych przechowujący taką właśnie informację. Po wyłonieniu kadydatów są oni umieszczani w jednym zbiorze. Zbiór ten trafia do wspólnego drzewa haszowego. W tym kroku modyfikowane są również odpowiednie falgi. Samo generowanie kandydatów i selekcja zbiorów czestych nadal realizowane sa odrebnie dla poszczególnych zapytań. Zliczany jest natomiast zintegrowany zbiór kandydatów. Podczas tej fazy brani są pod uwagę tylko kandydaci wygenerowani przez zapytania odwołujące się do aktualnie odczytywanej partycji bazy danych i w przypadku gdy kandydat zawiera się w przetwarzanej transakcji, to zwiększa się liczniki kandydatów związne z tymi zapytaniami.
Eksperymenty \cite{WojciechwskiCCT} pokazały, że jest to alogrytm wydajniejszy i lepiej skalowany od Common Counting.